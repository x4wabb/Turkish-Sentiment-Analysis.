{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UALRl-3LRBj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "matplotlib.rcParams[\"figure.figsize\"] = (20,10)\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import sklearn\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6hmYXeUc_T_l",
        "outputId": "704f3ff2-0b56-4145-db27-ea3369764581"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e45YD1VE_i7S"
      },
      "outputs": [],
      "source": [
        "## loading datasets train and test\n",
        "train_data = pd.read_csv('/content/sample_data/train.csv')\n",
        "test_data = pd.read_csv('/content/sample_data/train.csv')\n",
        "\n",
        "## loading stopwords dataset\n",
        "with open('/content/sample_data/turkce_stopwords.json', 'r', encoding='utf-8') as file:\n",
        "    stopwords_data = json.load(file)\n",
        "turkish_stopwords = set(stopwords_data['stopwords'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikaV2BucANvq",
        "outputId": "e0e4c053-7678-4d82-8c4e-1d2458d6f93c"
      },
      "outputs": [],
      "source": [
        "## showing train dataset\n",
        "print(\"Train Data Sample:\")\n",
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG04wHwVFatH"
      },
      "outputs": [],
      "source": [
        "################################################# tensorflow ###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L19NELKBFaKk",
        "outputId": "82760b03-0d40-4731-b7e4-eed0d47c4f87"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.sample(frac=0.5, random_state=42)\n",
        "test_data = test_data.sample(frac=0.5, random_state=42)\n",
        "\n",
        "# Preprocessing text: Lowercase, remove stopwords, and remove punctuation\n",
        "def preprocess_text(text, stop_words):\n",
        "    # Remove punctuation using regex\n",
        "    text = text.lower() # Lowercase text first\n",
        "    text = \"\".join([char for char in text if char.isalnum() or char.isspace()])  # Keep only alphanumeric and spaces\n",
        "    # Split the text into words\n",
        "    words = text.split()  # Split into a list of words\n",
        "    # Remove stopwords\n",
        "    filtered_words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    # Join back the words into a sentence\n",
        "    return ' '.join(filtered_words)  # Join back into a single string\n",
        "\n",
        "print(\"\\nPreprocessing text data...\")\n",
        "train_data['processed_text'] = train_data['text'].apply(lambda x: preprocess_text(x, turkish_stopwords))\n",
        "test_data['processed_text'] = test_data['text'].apply(lambda x: preprocess_text(x, turkish_stopwords))\n",
        "print(\"Datasets are ready!\")\n",
        "\n",
        "# Tokenization and vectorization using TensorFlow\n",
        "max_features = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(train_data['processed_text'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train_data['processed_text'])\n",
        "X_test = tokenizer.texts_to_sequences(test_data['processed_text'])\n",
        "\n",
        "# Pad sequences to ensure consistent length\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=100, padding='post')\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=100, padding='post')\n",
        "\n",
        "# Convert labels to categorical format\n",
        "label_map = {'Negative': 0, 'Notr': 1, 'Positive': 2}\n",
        "y_train = train_data['label'].map(label_map).values\n",
        "y_test = test_data['label'].map(label_map).values\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)\n",
        "\n",
        "# Define the neural network model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=16, input_length=500),  # Reduced output_dim\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),  # Reduced units\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining the model...\")\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "y_test_classes = tf.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test_classes, y_pred_classes) * 100\n",
        "print(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
        "print(classification_report(y_test_classes, y_pred_classes))\n",
        "\n",
        "\n",
        "# Function to predict sentiment of new text\n",
        "def predict_sentiment(text):\n",
        "    processed_text = preprocess_text(text, turkish_stopwords).numpy().decode('utf-8')\n",
        "    vectorized_text = tokenizer.texts_to_sequences([processed_text])\n",
        "    padded_text = tf.keras.preprocessing.sequence.pad_sequences(vectorized_text, maxlen=100, padding='post')\n",
        "    prediction = model.predict(padded_text)[0]\n",
        "    sentiment_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    return sentiment_map[predicted_class], prediction\n",
        "# Test the model with new input\n",
        "print(\"\\nTest with new input...\")\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    processed_text = preprocess_text(text, turkish_stopwords)\n",
        "    vectorized_text = vectorizer.transform([processed_text]).toarray()\n",
        "    probabilities = model.predict(vectorized_text)[0]\n",
        "    sentiment_index = tf.argmax(probabilities).numpy()\n",
        "    sentiment_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "    predicted_sentiment = sentiment_map[sentiment_index]\n",
        "\n",
        "    return predicted_sentiment, probabilities\n",
        "\n",
        "\n",
        "user_input = input(\"Enter a Turkish sentence for sentiment analysis: \")\n",
        "try:\n",
        "    sentiment, probabilities = predict_sentiment(user_input)\n",
        "    print(f\"\\nSentiment: {sentiment}\")\n",
        "    print(\"Probabilities:\")\n",
        "    print(f\"  Negative: {probabilities[0]:.4f}\")\n",
        "    print(f\"  Neutral: {probabilities[1]:.4f}\")\n",
        "    print(f\"  Positive: {probabilities[2]:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing input: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHprHPDMFfLb"
      },
      "outputs": [],
      "source": [
        "######################################### NLTK ######################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTHOtrggAZS3"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.sample(frac=0.7, random_state=42)  # Use 20% of the training data\n",
        "\n",
        "## lowercasing, removing stopwords, and punctuation the text\n",
        "def preprocess_text(text, stop_words):\n",
        "    words = nltk.word_tokenize(text.lower())\n",
        "    processed_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "    return ' '.join(processed_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGbkXgSCBK64",
        "outputId": "7942baae-98a3-423f-9c63-73b563334c9a"
      },
      "outputs": [],
      "source": [
        "## creating new raw dataset\n",
        "print(\"\\nPreprocessing text data...\")\n",
        "train_data['processed_text'] = train_data['text'].apply(lambda x: preprocess_text(x, turkish_stopwords))\n",
        "test_data['processed_text'] = test_data['text'].apply(lambda x: preprocess_text(x, turkish_stopwords))\n",
        "print(\"datasets are ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AsHIbPZDXR_"
      },
      "outputs": [],
      "source": [
        "## tf-idf vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_data['processed_text']).toarray()\n",
        "X_test = vectorizer.transform(test_data['processed_text']).toarray()\n",
        "y_train = train_data['label']\n",
        "y_test = test_data['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "PCKwkkcjEYAk",
        "outputId": "fe97be4a-a21f-487c-d415-8e4cbca3945a"
      },
      "outputs": [],
      "source": [
        "## create model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "## evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "## graphics for accuracy\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
        "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "plt.title(f'Normalized Confusion Matrix (Accuracy: {accuracy:.2f}%)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z08xG66FSQz"
      },
      "outputs": [],
      "source": [
        "## predict the sentiment of a new text\n",
        "def predict_sentiment(text):\n",
        "    processed_text = preprocess_text(text, turkish_stopwords)\n",
        "    vectorized_text = vectorizer.transform([processed_text]).toarray()\n",
        "    prediction = model.predict(vectorized_text)[0]\n",
        "    probabilities = model.predict_proba(vectorized_text)[0]\n",
        "    sentiment_map = {'Negative': 'Negative', 'Notr': 'Neutral', 'Positive': 'Positive'}\n",
        "    return sentiment_map[prediction], probabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIKwcAhEF5jd",
        "outputId": "45309c17-13fd-4a13-f420-3f1b103c16f8"
      },
      "outputs": [],
      "source": [
        "## new text input\n",
        "print(\"\\nTest with new input...\")\n",
        "user_input = input(\"Enter a Turkish sentence for sentiment analysis: \")\n",
        "sentiment, probabilities = predict_sentiment(user_input)\n",
        "print(f\"\\nSentiment: {sentiment}\")\n",
        "print(\"Probabilities:\")\n",
        "print(f\"  Negative: {probabilities[0]:.4f}\")\n",
        "print(f\"  Neutral: {probabilities[1]:.4f}\")\n",
        "print(f\"  Positive: {probabilities[2]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCYT2dk8H6NO"
      },
      "outputs": [],
      "source": [
        "#############################  data process ########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK687x_xSn53"
      },
      "outputs": [],
      "source": [
        "\n",
        "file_path = \"/content/turkce_stopwords.json\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "stopwords = data[\"stopwords\"]\n",
        "stopwords_df = pd.DataFrame(stopwords, columns=[\"Stopwords\"])\n",
        "\n",
        "print(stopwords_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "gfhBd_7DLRBl",
        "outputId": "a8975075-dcb2-42e5-87ae-e5bfae6f5a28"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(r'/content/test.csv')\n",
        "df1.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezxKCroOLRBm",
        "outputId": "6ac257a0-f17d-4668-a577-6b0615e6c683"
      },
      "outputs": [],
      "source": [
        "df1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm_2aYp-LRBm",
        "outputId": "32408eda-d0a8-41b3-82b8-ffba7f6dd314"
      },
      "outputs": [],
      "source": [
        "df1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHWz0aJ-mlOA",
        "outputId": "cfe40ed2-4c63-40a2-e086-57e69115fd26"
      },
      "outputs": [],
      "source": [
        "list(df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvE496rDnHMy",
        "outputId": "99d34b67-5e63-4075-d954-0e4f166049bd"
      },
      "outputs": [],
      "source": [
        "df1['label'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "rv1pcBnwLRBn",
        "outputId": "f167bb4c-2ca7-4e36-f4a5-62c085d9652c"
      },
      "outputs": [],
      "source": [
        "df1['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQmhN3iLnd4g",
        "outputId": "7af4f5cb-35fa-4e5a-b65e-746ab16674fe"
      },
      "outputs": [],
      "source": [
        "df2 = df1.drop(['text'], axis='columns')\n",
        "df2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "thxASiDuLRBo",
        "outputId": "68554bbd-db6c-4f2b-bcf2-14da363e1323"
      },
      "outputs": [],
      "source": [
        "df2.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "rrh6sixSoLwy",
        "outputId": "991d7844-fa6c-4dc6-df6b-20f280148e7e"
      },
      "outputs": [],
      "source": [
        "df3=df2.dropna()\n",
        "df3.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nAXXD_WtonXm",
        "outputId": "8460b7a2-e508-422a-912e-837c39cbbc84"
      },
      "outputs": [],
      "source": [
        "df3.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgiPeuWXrg99"
      },
      "outputs": [],
      "source": [
        "################## train ######################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "5g0K1uz3LRBo",
        "outputId": "7c22fa9b-48b6-422b-b8ca-af3099f48d8c"
      },
      "outputs": [],
      "source": [
        "df4 = pd.read_csv(r'/content/train.csv')\n",
        "df4.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH0WhKYqLRBp",
        "outputId": "81a507c2-fd6a-414c-886d-fcfbc4aba68e"
      },
      "outputs": [],
      "source": [
        "df4.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX_f3bsALRBp",
        "outputId": "c783d250-0d43-4271-acad-cad38e2a428f"
      },
      "outputs": [],
      "source": [
        "df4.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fyn0S0LrALM",
        "outputId": "0d8a8266-0cf5-4155-ce29-149223f2065a"
      },
      "outputs": [],
      "source": [
        "df4['text'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "qRWFYMxYLRBp",
        "outputId": "26c37eaa-1f6f-4a04-9591-940bd8b6d72e"
      },
      "outputs": [],
      "source": [
        "df4['text'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqStelR4rLPS",
        "outputId": "858e9d71-5242-494b-adfd-63d838785654"
      },
      "outputs": [],
      "source": [
        "df4['dataset'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "_boIa4XFLRBp",
        "outputId": "1b4b0133-bd1a-4d41-821f-8821f8382dab"
      },
      "outputs": [],
      "source": [
        "df4['dataset'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUA9Un_Pr6wo",
        "outputId": "fb37e21f-e4a9-4da3-e460-fb73cf0a48ca"
      },
      "outputs": [],
      "source": [
        "df4['label'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "eAY2Fwq3LRBq",
        "outputId": "b0609b83-18d3-4198-ebeb-45dfed8a48f3"
      },
      "outputs": [],
      "source": [
        "df4['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "wH_4x4R0LRBq",
        "outputId": "176826c4-236c-4601-af12-c1124f3fb977"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=df4, x='label', palette=\"viridis\")\n",
        "plt.title(\"Sentiment Distribution\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAC7m5dvslON"
      },
      "outputs": [],
      "source": [
        "############################# FEature engineering #############################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sQCI4dyFsrPz",
        "outputId": "96d91fb8-ecb7-4a2a-d8af-e6b6b5cf129d"
      },
      "outputs": [],
      "source": [
        "df4 = df4.drop(['dataset'], axis='columns')\n",
        "df4.shape\n",
        "df4.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiH_xY0KyjTQ"
      },
      "outputs": [],
      "source": [
        "df4_po = df4[df4['label'] == 'Positive']\n",
        "df4_ne = df4[df4['label'] == 'Negative']\n",
        "df4_not = df4[df4['label'] == 'Notr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bilJHV-uz4TC"
      },
      "outputs": [],
      "source": [
        "df4_po.to_csv(\"positive_sentiments.csv\", index=False)\n",
        "df4_ne.to_csv(\"negative_sentiments.csv\", index=False)\n",
        "df4_not.to_csv(\"notr_sentiments.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
